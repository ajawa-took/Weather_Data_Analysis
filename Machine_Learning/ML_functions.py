#import requests
import pandas as pd
#import json
import datetime
import numpy as np
# for position API
#import http.client, urllib.parse

# ML imports
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from math import radians, cos, sin
from sklearn.cluster import KMeans
from scipy.spatial.distance import cdist
# import hvplot.pandas

# my own data-getting, data-cleaning code
import GetCleanData as gcda

# some lat/long pairs for testing:
# get_geo_json('ML', 'timbuktu')        # 21.488625, -3.859508
# get_geo_json('IN', 'kolkata')         # 22.51829, 88.362951
# get_geo_json('US', 'San Francisco')   # 37.778008, -122.431272
# get_geo_json('us', 'chicago')         # 41.87897,-87.66063
# get_geo_json('ru', 'moscow')          # 55.741469, 37.615561

#   Three main functions:
#   step1 gets data from API via GetCleanData and date-chunkifies it
#   predictions takes in the output of step 1 and modeling parameters
#    and returns a dataframe with circle-coordinates of datechunks
#    and columns of labels generated by the models with given parameters:
#    comps is the number of PCA components used
#    randosta is the random state used by the clustering models
#    klist and wlist must have an entry for each model to be run:
#    k is the number of clusters, w is the weight given to date-coordinates
#   plot_predictions takes same inputs, returns nothing,
#    and prints a bunch of pretty plots, and least in a jupyter notebook


## rem is 0 for days 1-10, 1 for days 11-20, and 2 for days 21-
## the function returns 1 for 01/01-01/10, 6 for 02/21-02/29, and so on.
def chunkify(date):
    rem = min( (date.day-1) // 10, 2)
    return ( date.month *3  + rem -2 )

def step1(latitude, longitude, start_year, end_year):
    daf = gcda.get_clean_weather(latitude, longitude, start_year, end_year)
    keepers = ['pure_date', 'humid_avg', 'wind_high', 'median_wind',
               'cloud_4', 'cloud_12','cloud_20', 'precipitation_hours', 'rain_sum',
               'snowfall_sum', 'temperature_2m_max', 'temperature_2m_min']
    daf = daf[keepers]
    shorter_names = {'temperature_2m_max': 'temp_max', 'temperature_2m_min':'temp_min'}
    daf.rename(columns=shorter_names, inplace=True)
    # chunkify dates
    daf['date_chunk'] = daf["pure_date"].map(chunkify)
    daf['year'] = daf["pure_date"].map(lambda x: x.year)
    daf.drop('pure_date', axis=1, inplace=True)
    # aggregate averages
    avg_df = daf.groupby(['year', 'date_chunk']).agg(np.mean)
    avg_df.reset_index(inplace=True)
    # aggregate standard deviations
    dev_df = daf.groupby(['year', 'date_chunk']).agg(np.std)
    dev_df.reset_index(inplace=True) 
    # join the two aggregates
    unscaled_df = avg_df.join(dev_df, rsuffix='_dev', lsuffix='_avg')
    # cosmetic post-join cleanup
    unscaled_df.drop(['year_dev', 'date_chunk_dev'], axis=1, inplace=True)
    unscaled_df.rename(columns = {'year_avg':'year', 'date_chunk_avg': 'date_chunk'}, inplace = True)
    return unscaled_df

def predictions(unscaled_df, comps, klist, wlist, randosta):
    # rescale and PCA weather data, put date-chunks on a circle
    for_clusters_df = step2(unscaled_df, comps)
    day_labels_df = get_day_labels(for_clusters_df, klist, wlist, randosta)
    day_labels_df['date_chunk'] = unscaled_df['date_chunk']
    return get_modes(day_labels_df)

def plot_predictions(unscaled_df, comps, klist, wlist, randosta):
    modes = predictions(unscaled_df, comps, klist, wlist, randosta)
    plot_labels(modes)


# unscaled_df is theoutput of step1
# comps is the number of principal components kept

def step2(unscaled_df, comps):
    # scale weather data, forgetting dates
    scaled_df = rescale(unscaled_df)
    # run PCA on scaled weather data
    pcs_df = make_pca(scaled_df, comps)
    # convert integer date chunks to coordinates of points on a circle
    date_coord_df = dates_to_circle(unscaled_df[['date_chunk']])
    # combine PCA output with dates
    pcs_df['xchunk'] = date_coord_df['xchunk']
    pcs_df['ychunk'] = date_coord_df['ychunk']
    # return stuff ready for clustering
    return pcs_df

# input has datechunks, output only has circle-coordinates
# date chunks z run from 0 to 35; so 10*z in degrees is equispaced on circle


def dates_to_circle(date_chunks_df):
    daf = date_chunks_df.copy()
    # put date chunks on circle
    daf['xchunk'] =     daf['date_chunk'].map(lambda z: cos(radians(10*z)) )
    daf[
        'ychunk'] =     daf['date_chunk'].map(lambda z:  sin(radians(10*z)) )
    # drop original integers
    # daf.drop('date_chunk', axis=1, inplace=True)
    return daf

# # table lookup for translating back without arccos, arcsin
# def unchunk_lookup():
#     # make dataframe with values 0,1,2, ... , 34, 35
#     da_chunks = pd.DataFrame({'date_chunk': [i for i in range(36)]})
#     # add date-coord columns
#     return  dates_to_circle(da_chunks)

# ### Rescale and PCA weather data
# #### description
# averages are rescaled to send "reasonable" ranges [0,max] to [0,1] by $x \mapsto \frac{x}{max}$<br>
# standard deviations are rescaled with the minmax rescaler <br>
# #### questions
# previous iteration in aggregation_playground notebook used [-1,1] and StandardScaler <br>
# date-chunks-on-circle are not included -- why not? <br>
# is data in [0,1] instead of [-1, 1] a problem for feeding into PCA and then K-Means?

# input: dataframe to be scaled, shaped like output of step1
# output: scaled dataframe with no date info, ready to be fed into PCA

def rescale(scaling_df):
    # build scaled_df with only weather
    #   auto-rescale standard deviation columns with MinMaxScaler
    dev_cols = ['temp_max_dev', 'temp_min_dev', 'rain_sum_dev', 'snowfall_sum_dev',
                'precipitation_hours_dev', 'humid_avg_dev', 'median_wind_dev',
                'wind_high_dev', 'cloud_4_dev', 'cloud_12_dev', 'cloud_20_dev']
    X = scaling_df[dev_cols]
    scaler = MinMaxScaler()
    scaledata = scaler.fit_transform(X)
    # make new df to hold scaled data
    scaled_df = pd.DataFrame(scaledata, columns = dev_cols)
    #   hand-rescale averages to send "normal" ranges [0,max] to [0, 1]
    avg_maxes = {'temp_max_avg':100, 'temp_min_avg':100, 'humid_avg_avg':100,
                 'cloud_4_avg':100, 'cloud_12_avg':100, 'cloud_20_avg':100,
                 'precipitation_hours_avg':24, 'rain_sum_avg':0.5 , 'snowfall_sum_avg':12,
                 'median_wind_avg':15, 'wind_high_avg':25}
    # rescale averages, add them into new df
    for colnam in avg_maxes.keys():
        scaled_df[colnam] = scaling_df[colnam].map(lambda x: (x / avg_maxes[colnam]) )
    return scaled_df 

# generating column names for the dtaframe containing components
def pc_cols(k):
    out = []
    for i in range(k):
        out.append('PC '+str(i))
    return out

# the pcs2_df returned differs from original notebook pcs_df: the new one has datechunks
def make_pca(scaled_df, comps):
    pca = PCA(n_components=comps)
    pca_out = pca.fit_transform(scaled_df)
    pcs2_df = pd.DataFrame(pca_out, columns = pc_cols(comps))
    # next two lines for debugging, remove in production!!
    for k in range(5):
        print(5+k, "principal components explain", sum(pca.explained_variance_ratio_[:5+k]))
    return pcs2_df

# this function adds one column to day_labels_df
# the column is the output of K-Means cluster algorithm run on
# the data in for_clusters_df with random state = randosta, and
# k clusters, date-coordinates weighted by w
# the columns is named for example "season k4 w1.2 r17" when
# there are 4 clusters, weight of dates is 1.2, and random state is 17

def label_days(day_labels_df, for_clusters_df, k, w, randosta):
    colnam = f'season k{k} w{w} r{randosta}'
    km = KMeans(n_clusters=k, random_state=randosta)
    # scale down date-coord columns by w
    for col in ['xchunk', 'ychunk']:
        for_clusters_df[col] = for_clusters_df[col]*w
    km.fit(for_clusters_df)
    day_labels_df[colnam] = km.labels_
    # reset date-coord columns
    for col in ['xchunk', 'ychunk']:
        for_clusters_df[col] = day_labels_df[col]
    return day_labels_df

# klist, wlist must be same lenth!
# klist must be full of integers
def get_day_labels(for_clusters_df, klist, wlist, randosta):
    day_labels_df = for_clusters_df[['xchunk', 'ychunk']].copy()
    for i in range(len(klist)):
        label_days(day_labels_df, for_clusters_df, klist[i], wlist[i], randosta)
    return day_labels_df

# aggregates across years using mode
def get_modes(day_labels_df):
    modes = day_labels_df.groupby(['xchunk', 'ychunk']).agg(lambda x: x.mode().iloc[0])
    modes.reset_index(inplace=True)
    modes.sort_values(by = 'date_chunk', inplace=True)
    modes.set_index('date_chunk', inplace=True)
    return modes

def plot_labels(modes):
    for colnam in modes.columns:
        if ((colnam == 'xchunk' or colnam == 'ychunk') or colnam == 'date_chunk'):
            pass
        else:
            modes.plot.scatter(x='xchunk', y='ychunk', c=colnam, s=100, colormap='Set1')

# smoothing functions

# getting averages for one seson
def season_avgs(unscaled_df, modes, model_name, k):
    output = pd.DataFrame(columns=unscaled_df.columns)
    zerow = pd.DataFrame(0, index = ['x'], columns=unscaled_df.columns)
    temp_df = modes[[model_name]].reset_index()
    leests = temp_df.groupby(model_name).aggregate(lambda x: tuple(x))
    for i in range(k):
        if i in leests.index.to_list():
            leest = leests['date_chunk'].loc[i]
            desc = unscaled_df[unscaled_df['date_chunk'].isin(leest)].describe()
            new_row = desc[desc.index=='mean']
        else: 
            new_row = zerow.copy()
        new_row.index = [i]
        output = pd.concat([output,new_row])
    return output

# getting distances between seasons
def dmatrix(unscaled_df, modes, model_name, k):
    season_avg_df = season_avgs(unscaled_df, modes, model_name, k)
    season_avg_df.drop(['year','date_chunk'], axis=1, inplace=True)
    # maybe use ‘cityblock’ metric to speed up?
    # print(season_avg_df.iloc[:,1:])
    return cdist(season_avg_df.iloc[:,1:], season_avg_df.iloc[:,1:], metric='euclidean')

# smoothing out one model
def smooth_one(unscaled_df, modes, model_name, k):
    # making two diff lists, for easy wrapping indices
    old_modes = modes[model_name].to_list()
    new_modes = modes[model_name].to_list()
    dist_matrix = dmatrix(unscaled_df, modes, model_name, k)
    # fill in embedded singleton
    for i in range(len(new_modes)):
        if new_modes[i-2] == new_modes[i]:
            new_modes[i-1] = new_modes[i-2]
    # more controversially, "fix" the middle of 3 different consecutive
    # pick the one of the adjacent news closest to the here-original
    for i in range(len(new_modes)):
        if len({new_modes[i-2], new_modes[i-1], new_modes[i]}) == 3:
            if dist_matrix[new_modes[i-2]][old_modes[i-1]] <= \
            dist_matrix[new_modes[i]][old_modes[i-1]]:
                new_modes[i-1] = new_modes[i-2]
            else:
                new_modes[i-1] = new_modes[i]
    # initialize to account for expected boundaries
    guilt = -30*len(set(new_modes))
    for i in range(len(new_modes)):
        guilt += dist_matrix[new_modes[i]][old_modes[i]]
        if new_modes[i-1] != new_modes[i]:
            # each extra boundary is as bad as a distance-30 switch
            guilt += 30
    return {'model_name': model_name, 'new_modes': new_modes, 'guilt': guilt, 'k': k}

# given a bunch of models, smoothes all, and picks the one that 
# needed the least fixing and has fewest boundaries after smoothing
def pick_model(unscaled_df, modes):
    models = []
    for colnam in modes.columns:
        if ((colnam == 'xchunk' or colnam == 'ychunk') or colnam == 'date_chunk'):
            pass
        else:
            models.append(smooth_one( unscaled_df, modes, colnam, int(colnam[8]) ))
    print([(x['guilt'], x['model_name']) for x in models])
    models.sort(key= (lambda x: x['guilt']))
    return models[0]

# finally, takes in unscaled weather data, and a bunch of models
# picks best model, returns that model's
# seasons labels and weather avgs for each season
def smooth_seasons(unscaled_df, modes):
    smoothest = pick_model(unscaled_df, modes)
    season_labels_df  = modes[['xchunk', 'ychunk']].copy()
    season_labels_df['season'] = smoothest['new_modes']
    season_stats_df = season_avgs(unscaled_df, season_labels_df, 'season', smoothest['k'])
    season_stats_df.drop(['year', 'date_chunk'], axis=1, inplace=True)
    return {'labels': season_labels_df, 'stats': season_stats_df}



































